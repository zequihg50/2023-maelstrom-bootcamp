{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AP1: Hands-on tutorial 3: Large-scale training\n",
    "This tutorial covers how we can train ML-models efficiently on HPC systems.\n",
    "\n",
    "The xarray dataset we have been using so far works well for small datasets. Problems occur with larger datasets that cannot fit into main memory on the system. In these cases we need to stream the data from disk, discarding it when it has been processed. Typically, one can use the tensorflow.Dataset class for this. The data loader we have implemented can provide this by calling the `loader.get_dataset()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cd maelstrom && pip install -e . # no internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'maelstrom' has no attribute 'loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmaelstrom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m datadir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/p/scratch/deepacf/maelstrom/maelstrom_data/ap1/air_temperature/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mmaelstrom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241m.\u001b[39mget({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilenames\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatadir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/5GB/2020030*T*Z.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalization\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatadir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/normalization.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget_dataset()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(dataset))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'maelstrom' has no attribute 'loader'"
     ]
    }
   ],
   "source": [
    "import maelstrom\n",
    "\n",
    "datadir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap1/air_temperature/\"\n",
    "loader = maelstrom.loader.get({\"type\": \"file\", \"filenames\": [f\"{datadir}/5GB/2020030*T*Z.nc\"], \"normalization\":f\"{datadir}/normalization.yml\", \"cache_size\": 0, \"debug\": True})\n",
    "\n",
    "dataset = loader.get_dataset()\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorflow dataset is a generator function, which is an object we can iterate over and load the data on-the-fly. The following for loop iterates over the dataset, loading one file at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for predictors, target in dataset:\n",
    "    print(predictors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A keras model will recognize this type of dataset when training, just like in the case of an xarray. Unlike the xarray dataset, the tensorflow dataset contains both the predictors and the targets, therefore we only need to provide one argument to `fit`: `model.fit(dataset)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scripts for large-scale runs\n",
    "To do a large scale run, we need 1) a python script that will perform the training of the model, and 2) a submission script written in bash. Her is an example python script, which you can also find in `job_script.py`. We will not run this cell here, since it will take too long so make sure you save your script in `job_script.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import maelstrom\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import xarray as xr\n",
    "\n",
    "# Get model name from command-line argument\n",
    "if len(sys.argv) != 2:\n",
    "    raise Exception(\"This script needs one input argument (the model name, one of cnn, separable, lstm, unet)\")\n",
    "\n",
    "model_name = sys.argv[1]\n",
    "\n",
    "datadir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap1/air_temperature/\"\n",
    "# Set up the output directory, where results will go\n",
    "user = os.environ[\"USER\"]\n",
    "outputdir = f\"/p/scratch/training2223/{user}/results/{model_name}\"\n",
    "try:\n",
    "    os.mkdir(outputdir)\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# List available GPUs (for debugging)\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "# Set up the loader\n",
    "debug = True\n",
    "if not debug:\n",
    "    # Full scale testing\n",
    "    cache_size = 0\n",
    "    # Here we load every second day to reduce the amount of data to read\n",
    "    loader_train = maelstrom.loader.get({\"type\": \"file\", \n",
    "                                         \"filenames\": [f\"{datadir}/5TB/2020???[13579]T*Z.nc\", f\"{datadir}/5TB/20210[1-2]?[13579]T*Z.nc\"],\n",
    "                                         \"normalization\": f\"{datadir}/normalization.yml\", \"predict_diff\": True, \"debug\": debug, \"patch_size\": 256, \"cache_size\": cache_size})\n",
    "    loader_test = maelstrom.loader.get({\"type\": \"file\", \n",
    "                                        \"filenames\": [\n",
    "                                                f\"{datadir}/5TB/20210[3-9]?[13579]T*Z.nc\",\n",
    "                                                f\"{datadir}/5TB/20211??[13579]T*Z.nc\",\n",
    "                                                f\"{datadir}/5TB/20220[1-3]?[13579]T*Z.nc\"],\n",
    "                                            \"normalization\": f\"{datadir}/normalization.yml\", \"predict_diff\": True, \"debug\": debug, \"patch_size\": 256, \"cache_size\": cache_size})\n",
    "else:\n",
    "    # Use this section for debugging\n",
    "    cache_size = 0\n",
    "    loader_train = maelstrom.loader.get({\"type\": \"file\", \n",
    "                                         \"filenames\": [f\"{datadir}/5GB/20200301T*Z.nc\"],\n",
    "                                         \"normalization\": f\"{datadir}/normalization.yml\", \"predict_diff\": True, \"debug\": debug, \"patch_size\": 32, \"cache_size\": cache_size})\n",
    "    loader_test = maelstrom.loader.get({\"type\": \"file\", \n",
    "                                        \"filenames\": [\n",
    "                                                f\"{datadir}/5GB/2021020*T*Z.nc\"],\n",
    "                                            \"normalization\": f\"{datadir}/normalization.yml\", \"predict_diff\": True, \"debug\": debug, \"patch_size\": 32, \"cache_size\": cache_size})\n",
    "print(loader_train)\n",
    "print(loader_test)\n",
    "train_dataset = loader_train.get_dataset()\n",
    "test_dataset = loader_test.get_dataset()\n",
    "\n",
    "quantiles = [0.1,0.5,0.9]\n",
    "loss = lambda x, y: maelstrom.loss.quantile_score(x, y, quantiles)\n",
    "\n",
    "input_shape = loader_train.predictor_shape\n",
    "num_outputs = len(quantiles)\n",
    "\n",
    "# Create your model here\n",
    "if model_name == \"cnn\":\n",
    "    model = keras.Sequential([\n",
    "            keras.layers.Input(input_shape),\n",
    "            keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\"),\n",
    "            keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\"),\n",
    "            keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\"),\n",
    "            keras.layers.Dense(num_outputs)\n",
    "        ])\n",
    "elif model_name == \"lstm\":\n",
    "    model = keras.Sequential([\n",
    "            keras.layers.Input(input_shape),\n",
    "            keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\"),\n",
    "            keras.layers.ConvLSTM2D(9, 3, padding=\"same\", activation=\"relu\", return_sequences=True),\n",
    "            keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\"),\n",
    "            keras.layers.Dense(num_outputs)\n",
    "        ])\n",
    "elif model_name == \"unet\":\n",
    "    num_levels=3\n",
    "    num_features=16\n",
    "    pool_size=2\n",
    "    conv_size=3\n",
    "\n",
    "    inputs = keras.layers.Input(input_shape)\n",
    "    levels = list()\n",
    "\n",
    "    pool_size = [1, pool_size, pool_size]\n",
    "    conv_size = [1, conv_size, conv_size]\n",
    "\n",
    "    Conv = keras.layers.Conv3D\n",
    "\n",
    "    # Downsampling\n",
    "    # conv -> conv -> max_pool\n",
    "    outputs = inputs\n",
    "    for i in range(num_levels - 1):\n",
    "        outputs = Conv(num_features, conv_size, activation=\"relu\", padding=\"same\")(\n",
    "            outputs\n",
    "        )\n",
    "        outputs = Conv(num_features, conv_size, activation=\"relu\", padding=\"same\")(\n",
    "            outputs\n",
    "        )\n",
    "        levels += [outputs]\n",
    "\n",
    "        outputs = keras.layers.MaxPooling3D(pool_size=pool_size)(outputs)\n",
    "        num_features *= 2\n",
    "\n",
    "    # conv -> conv\n",
    "    outputs = Conv(num_features, conv_size, activation=\"relu\", padding=\"same\")(\n",
    "        outputs\n",
    "    )\n",
    "    outputs = Conv(num_features, conv_size, activation=\"relu\", padding=\"same\")(\n",
    "        outputs\n",
    "    )\n",
    "\n",
    "    # upconv -> concat -> conv -> conv\n",
    "    for i in range(num_levels - 2, -1, -1):\n",
    "        num_features /= 2\n",
    "        outputs = keras.layers.Conv3DTranspose(num_features, conv_size, strides=pool_size, padding=\"same\")(outputs)\n",
    "\n",
    "        outputs = keras.layers.concatenate((levels[i], outputs), axis=-1)\n",
    "        outputs = Conv(num_features, conv_size, activation=\"relu\", padding=\"same\")(\n",
    "            outputs\n",
    "        )\n",
    "        outputs = Conv(num_features, conv_size, activation=\"relu\", padding=\"same\")(\n",
    "            outputs\n",
    "        )\n",
    "\n",
    "    # Dense layer at the end\n",
    "    outputs = keras.layers.Dense(num_outputs, activation=\"linear\")(\n",
    "        outputs\n",
    "    )\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "elif model_name == \"separable\":\n",
    "    inputs = keras.layers.Input(input_shape)\n",
    "    input_temp = inputs[..., 0:3]\n",
    "    input_topo = keras.layers.concatenate([tf.expand_dims(inputs[..., i], -1) for i in [8, 11, 12, 13]], -1)\n",
    "    input_other = keras.layers.concatenate([tf.expand_dims(inputs[..., i], -1) for i in [3, 4, 5, 6, 7, 9, 10]], -1)\n",
    "    input_temp = keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\")(input_temp)\n",
    "    input_temp = keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\")(input_temp)\n",
    "    input_topo = keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\")(input_topo)\n",
    "    input_topo = keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\")(input_topo)\n",
    "    input_other = keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\")(input_other)\n",
    "    input_other = keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\")(input_other)\n",
    "    merged = keras.layers.concatenate((input_temp, input_topo, input_other), axis=-1)\n",
    "    merged = keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\")(merged)\n",
    "    merged = keras.layers.Conv3D(9, [3, 3, 3], activation=\"relu\", padding=\"same\")(merged)\n",
    "    outputs = keras.layers.Conv3D(num_outputs, [3, 3, 3], activation=\"linear\", padding=\"same\")(merged)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "else:\n",
    "    raise Exception(\"Model name must be one of cnn, separable, lstm, unet\")\n",
    "    \n",
    "optimizer = keras.optimizers.Adam(learning_rate=1.0e-3)\n",
    "model.compile(optimizer, loss=loss)\n",
    "model.summary()\n",
    "\n",
    "# Add a callback to fit to save the model state after every 63 batches.\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(f\"{outputdir}/model_results/checkpoint\", save_freq=63, save_weights_only=False, monitor=\"loss\", verbose=True)]\n",
    "\n",
    "print(\"\\n### Starting training\")\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "s_time = time.time()\n",
    "history = model.fit(train_dataset, batch_size=batch_size, epochs=epochs, callbacks=callbacks) # validation_data=val_dataset, \n",
    "print(f\"Training time {model_name} \", time.time() - s_time)\n",
    "\n",
    "# Perform inference on the test dataset and store loss to file\n",
    "print(\"\\n### Starting test evaluation\")\n",
    "num_times = len(loader_test)\n",
    "num_leadtimes = len(loader_test.leadtimes)\n",
    "test_loss = np.zeros([num_times, num_leadtimes])\n",
    "s_time = time.time()\n",
    "samples_per_file = loader_test.num_samples_per_file * loader_test.num_patches_per_sample\n",
    "\n",
    "# Loop over each batch in the dataset\n",
    "for i, (predictors, targets) in enumerate(test_dataset):\n",
    "    print(predictors.shape, targets.shape)\n",
    "    current_time = loader_test.times[i // samples_per_file]\n",
    "    time_index = i // samples_per_file\n",
    "    for s in range(predictors.shape[0]):\n",
    "        output = model.predict_on_batch(tf.expand_dims(predictors[s, ...], 0))\n",
    "        for j in range(num_leadtimes):\n",
    "            current_loss = loss(targets[:, j, ...], output[:, j, ...])\n",
    "            test_loss[time_index, j] += current_loss\n",
    "    print(f\"Done time {i}\", time.time() - s_time)\n",
    "\n",
    "for t in range(num_times):\n",
    "    test_loss[t, :] /= samples_per_file\n",
    "    print(test_loss)\n",
    "\n",
    "print(\"Overall test loss:\", np.mean(test_loss))      \n",
    "for j in range(num_leadtimes):\n",
    "    print(f\"   Test loss for leadtime {loader_test.leadtimes[j]/3600} h:\", np.mean(test_loss[:, j]))\n",
    "\n",
    "# Store model results in a NetCDF file\n",
    "results_dataset = xr.Dataset(coords={\"time\": ([\"time\"], loader_test.times, {\"units\": \"seconds since 1970-01-01 00:00:00 +00:00\"}), \"leadtime\": ([\"leadtime\"], loader_test.leadtimes, {\"units\": \"seconds\"})}, data_vars={\"loss\": ((\"time\", \"leadtime\"), test_loss)})\n",
    "results_dataset.to_netcdf(f\"{outputdir}/test_results.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write the job submission script. This contains options for the scheduling system, will load the run environment, and will call the python script. You can find an example script in `job.sh`, which we repeat for you here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "#!/bin/bash -x\n",
    "#SBATCH --job-name=maelstrom_training\n",
    "#SBATCH --account=straining2223 # Do not change the account name\n",
    "#SBATCH --nodes=1                                                                               \n",
    "#SBATCH --ntasks=1                                                                              \n",
    "#SBATCH --cpus-per-task=12                                                                       \n",
    "\n",
    "#SBATCH --output=jewels-benchmark-out.%j                                        \n",
    "#SBATCH --error=jewels-benchmark-err.%j                                                    \n",
    "#SBATCH --time=04:00:00                                                                           \n",
    "#SBATCH --gres=gpu:1                                                                          \n",
    "#SBATCH --partition=booster                                                                     \n",
    "#SBATCH --mail-type=ALL                                                                         \n",
    "##SBATCH --mail-user=fill in your email here, and uncomment the line if you want an email sent to you\n",
    "\n",
    "# Load the environment\n",
    "source /p/project/training2223/venv_apps/venv_ap1/activate.sh\n",
    "\n",
    "# Run the job script\n",
    "srun python job_script.py $@\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting and monitoring a job\n",
    "To submit the job, run the following on the command-line when logged on to a Julich Booster login node. Make sure you are cd'ed into the directory where the script is located. The argument to the job is one of the model defined in the job that you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "[nipen1@jwlogin22 Day1]$ sbatch job.sh cnn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To check the status of your job, use the `squeue` command. The output of the script will appear in `jewels-benchmark-out.<job_id>`, and any errors are reported in `jewels-benchmark-out.<job_id>`.\n",
    "\n",
    "## Checking model results\n",
    "\n",
    "After the model has been trained and evaluated, you typically willl want to analyse the results further. The training script computed the scores for each time and leadtime and saved it in a NetCDF file. We can loader this file and analyse the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.load_dataset(f\"{outputdir}/test_results.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The training script also has a callback for saving the model weights as training progresses. Since we used a custom loss function, we have to tell the model loader the definition of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.1,0.5,0.9]\n",
    "loss = lambda x, y: maelstrom.loss.quantile_score(x, y, quantiles)\n",
    "\n",
    "model = keras.models.load_model(f\"{outputdir}\", custom_objects={\"<lambda>\": loss})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_bootcamp",
   "language": "python",
   "name": "venv_bootcamp3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6066b6710af8e73fb82b0a25b34aec48fb9431d8cd1e145138d738107b7009c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
